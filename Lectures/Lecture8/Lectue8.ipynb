{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJItMl779l9K"
   },
   "source": [
    "## **Lecture 8: Fine-tuning an LLM**  \n",
    "\n",
    "Fine-tuning is the process of taking a pre-trained large language model (LLM) and further training it on a specific dataset to improve its performance on a targeted task. This allows the model to specialize while leveraging the vast knowledge it has already acquired during its initial training phase. Fine-tuning is particularly useful for domain-specific applications, improving chatbot interactions, and enhancing model efficiency.  \n",
    "\n",
    "### **Workshop Information**  \n",
    "\n",
    "This notebook is based on [Link 1](https://goo.gle/gemma-ft-workshop), which was part of a workshop held at the **Google Cloud office**. The session was led by **Gus Martins**, a former **Google Senior Developer and Project Manager** who worked directly on the **Gemma 2 model**. He is now part of **Google DeepMind**, where he continues to contribute to cutting-edge AI research. As such, this notebook includes content that is licensed under the Apache License 2.0. You may use, modify, and distribute this work under the terms of the license. \n",
    "\n",
    "@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "Copyright 2024 Google LLC.\n",
    "\n",
    "\n",
    "\n",
    "### **Additional Resources**  \n",
    "For further learning, [Link 2](https://goo.gle/gemma-cookbook) provides a comprehensive set of resources to get started with **Gemma models**, covering topics such as fine-tuning, chatbot creation, and more.  \n",
    "\n",
    "To access **Gemma 2 models on Kaggle**, refer to [Link 3](https://www.kaggle.com/models/google/gemma-2).  \n",
    "\n",
    "For guidance on setting up **Gemma on Google Colab**, see [Link 4](https://ai.google.dev/gemma/docs/setup).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjHF030im6Mr"
   },
   "source": [
    "### **Reminder**  \n",
    "This code is designed to run on **Google Colab**. If you're not using Colab, please set the environment variables as appropriate for your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-OiJP8ODDi3"
   },
   "outputs": [],
   "source": [
    "# Download some important resources\n",
    "# !pip install fsspec==2024.9.0\n",
    "# !pip install -q -U keras-nlp datasets\n",
    "# !pip install -q -U keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jvzIiKpETA0F"
   },
   "outputs": [],
   "source": [
    "# Import relevant modules\n",
    "\n",
    "import os\n",
    "from google.colab import userdata, drive\n",
    "import time\n",
    "import keras_nlp\n",
    "import keras\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYH6dDdwhBYT"
   },
   "source": [
    "## **Understanding APIs in AI Development**  \n",
    "\n",
    "An **API (Application Programming Interface)** is a set of rules and protocols that allows different software applications to communicate with each other. APIs enable seamless integration of services, making it easier to access and use external functionalities without needing to understand their internal workings.  \n",
    "\n",
    "### **API in Action – Colab & Kaggle**  \n",
    "In the code snippet below:  \n",
    "\n",
    "```python\n",
    "os.environ['KAGGLE_USERNAME'] = \"KAGGLE_USERNAME\"\n",
    "os.environ['KAGGLE_KEY'] = \"KAGGLE_KEY\"\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "```\n",
    "`os.environ['KAGGLE_USERNAME']` and `os.environ['KAGGLE_KEY']` set environment variables to authenticate with the **Kaggle API**, which is used to download datasets and models.  \n",
    "\n",
    "`drive.mount(\"/content/drive\")` uses **Google Colab's API** to connect and store files in **Google Drive**, allowing persistent storage of artifacts.  \n",
    "\n",
    "APIs like these are essential in AI workflows, automating authentication, data access, and storage integration with minimal effort. 🚀  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45DJDytJAuPc",
    "outputId": "6a0deacd-7d10-4930-c8fa-a8c0fc4eaee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
    "# vars as appropriate for your system.\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = \"KAGGLE_USERNAME\"\n",
    "os.environ['KAGGLE_KEY'] = \"KAGGLE_KEY\"\n",
    "\n",
    "# Mounting gDrive for to store artifacts\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTa5YeZSiBHb"
   },
   "source": [
    "## **Set the backend, training configurations, and hyperparameters**\n",
    "\n",
    "This script configures the environment for training a model using the JAX backend with Keras. It ensures that JAX is set as the deep learning backend and optimizes memory usage to prevent fragmentation.  \n",
    "\n",
    "The training configurations define key hyperparameters:  \n",
    "- **`token_limit`** sets the maximum number of tokens per input.  \n",
    "- **`num_data_limit`** controls the number of data samples used.  \n",
    "- **`lora_name`** specifies the identifier for the LoRA (Low-Rank Adaptation) model.  \n",
    "- **`lora_rank`** determines the rank for LoRA fine-tuning, which affects parameter efficiency.  \n",
    "- **`lr_value`** sets the learning rate for optimization.  \n",
    "- **`train_epoch`** defines the number of training epochs.  \n",
    "- **`model_id`** indicates the specific model being fine-tuned, in this case, the Gemma 2 Instruct model.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fVQSwuJFDfmJ"
   },
   "outputs": [],
   "source": [
    "# Set the backbend before importing Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n",
    "\n",
    "\n",
    "# Training Configurations\n",
    "token_limit = 128\n",
    "num_data_limit = 100\n",
    "lora_name = \"my_lora\"\n",
    "lora_rank = 4\n",
    "lr_value = 1e-3\n",
    "train_epoch = 10\n",
    "model_id = \"gemma2_instruct_2b_en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De_YMltjipDh"
   },
   "source": [
    "## **Understanding the Model Architecture and Numbers**  \n",
    "\n",
    "This breakdown provides key insights into the structure and complexity of the **Gemma Causal Language Model (LLM)** and its tokenizer.  \n",
    "\n",
    "#### **Preprocessor**  \n",
    "- **`gemma_tokenizer` (GemmaTokenizer)**: The tokenizer converts text into numerical tokens using a **vocabulary size of 256,000**, meaning the model can recognize and process up to **256K unique tokens (words, subwords, or characters).**  \n",
    "\n",
    "#### **Model Components**  \n",
    "- **`padding_mask` (InputLayer)**: Handles padding to ensure uniform input sequence length. It has **0 parameters** since it does not require learning.  \n",
    "- **`token_ids` (InputLayer)**: Takes tokenized input sequences, also with **0 parameters.**  \n",
    "- **`gemma_backbone` (GemmaBackbone)**:  \n",
    "  - Outputs a tensor of shape **(None, None, 2304)**, meaning for each token, the model produces a **2304-dimensional embedding**.  \n",
    "  - Contains **2.61 billion parameters**, making it the largest component of the model. These parameters define the core neural network architecture.  \n",
    "- **`token_embedding` (ReversibleEmbedding)**:  \n",
    "  - Outputs a tensor of shape **(None, None, 256000)**, mapping token embeddings back into the vocabulary space.  \n",
    "  - Has **589.8 million parameters**, responsible for learning token representations.  \n",
    "\n",
    "#### **Total Parameters**  \n",
    "- **Total parameters**: **2.61 billion (≈9.74 GB of memory usage).**  \n",
    "- **Trainable parameters**: **All 2.61 billion are trainable**, meaning every part of the model can be fine-tuned.  \n",
    "- **Non-trainable parameters**: **0**, indicating there are no frozen layers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "1Q6RniZADnvZ",
    "outputId": "7c090520-d05d-4916-ecd1-804897e6a301"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id) # Load the model\n",
    "gemma_lm.summary() # Gemma 2 model summary\n",
    "\n",
    "tick_start = 0\n",
    "\n",
    "def tick():\n",
    "    global tick_start\n",
    "    tick_start = time.time()\n",
    "\n",
    "def tock():\n",
    "    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n",
    "\n",
    "\n",
    "# Format the model to complete a conversation (Prompt engineering)\n",
    "\n",
    "def text_gen(prompt):\n",
    "    tick()\n",
    "    input = f\"user\\n{prompt}\\nmodel\\n\"\n",
    "    output = gemma_lm.generate(input, max_length=token_limit)\n",
    "    print(\"\\nGemma output:\")\n",
    "    print(output)\n",
    "    tock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tE6oqmgTDwf4",
    "outputId": "de3c56e2-72d3-48de-8969-839e6f839d87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gemma output:\n",
      "user\n",
      "Make the following sentence more romantic.\n",
      "\"Hi, do you want to go out?\"\n",
      "model\n",
      "Here are a few ways to make the sentence \"Hi, do you want to go out?\" more romantic, depending on the tone you're going for:\n",
      "\n",
      "**Playful & Flirty:**\n",
      "\n",
      "* \"Fancy a night out? 😉\"\n",
      "* \"I was thinking of checking out [place]. Want to join me?\"\n",
      "* \"I'd love to take you out sometime. What do you say?\"\n",
      "\n",
      "**Sweet & Suggestive:**\n",
      "\n",
      "* \"I've been wanting to try that\n",
      "TOTAL TIME ELAPSED: 39.49s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Speak like a pirate. What is Saint Valentine's day\n",
      "model\n",
      "Ahoy, matey!  Ye be askin' 'bout Saint Valentine's Day, eh?  Shiver me timbers, it be a day for love, like a mermaid's song! \n",
      "\n",
      "It be a day for the landlubbers to show their love for their sweethearts, like a treasure chest full o' jewels.  They write letters, give gifts, and maybe even share a kiss, like a pirate kissin' a mermaid! \n",
      "\n",
      "But beware, me hearties!  Some landlubbers be\n",
      "TOTAL TIME ELAPSED: 1.10s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Write a love poem\n",
      "model\n",
      "Your laughter, a melody that dances in my ear,\n",
      "A symphony of joy, chasing away all fear.\n",
      "Your eyes, twin pools of starlight, shimmering bright,\n",
      "Reflecting a love that burns with endless light.\n",
      "\n",
      "Your touch, a featherlight caress, a gentle breeze,\n",
      "Whispering secrets only our hearts can seize.\n",
      "Your voice, a soothing balm, a lullaby so sweet,\n",
      "Guiding me through darkness, making my spirit complete.\n",
      "\n",
      "With every passing moment, my love for you grows strong,\n",
      "A vibrant tapestry woven, where we both belong.\n",
      "TOTAL TIME ELAPSED: 1.18s\n"
     ]
    }
   ],
   "source": [
    "# Inference before fine-tuning. Let's look at some examples!\n",
    "text_gen(\"Make the following sentence more romantic.\\n\\\"Hi, do you want to go out?\\\"\")\n",
    "text_gen(\"Speak like a pirate. What is Saint Valentine's day\")\n",
    "text_gen(\"Write a love poem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1nWakmbjkiH"
   },
   "source": [
    "### **What is a Tokenizer?**  \n",
    "\n",
    "A tokenizer converts human language into numerical tokens that AI models can process. It acts as a translator between words and the numerical format LLMs understand.  \n",
    "\n",
    "#### **Gemma Tokenizer**  \n",
    "- Based on **SentencePiece**, it learns optimal subword segmentation based on a **fixed 256K vocabulary**.  \n",
    "- Uses **byte-level encoding**, enabling support for all languages, including those with complex writing systems (e.g., Chinese, Japanese, Korean).  \n",
    "- A large vocabulary improves performance on diverse tasks, including multilingual text processing.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDm3uGJ5EgLN",
    "outputId": "d11047ec-9aaf-4fd7-8264-7c789a2cfb26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  2045    791   1125  10920 235269    970 160976 235269    832   1593\n",
      "   1450   4868    578  24615   2705 235265   1927    590   4880   2182\n",
      " 235269    692   6293    665    577    682 235269  14062    692 235341], shape=(30,), dtype=int32)\n",
      "  2045 -> You\n",
      "   791 ->  have\n",
      "  1125 ->  been\n",
      " 10920 ->  wonderful\n",
      "235269 -> ,\n",
      "   970 ->  my\n",
      "160976 ->  Juliette\n",
      "235269 -> ,\n",
      "   832 ->  all\n",
      "  1593 ->  through\n",
      "  1450 ->  these\n",
      "  4868 ->  dark\n",
      "   578 ->  and\n",
      " 24615 ->  violent\n",
      "  2705 ->  days\n",
      "235265 -> .\n",
      "  1927 ->  If\n",
      "   590 ->  I\n",
      "  4880 ->  needed\n",
      "  2182 ->  love\n",
      "235269 -> ,\n",
      "   692 ->  you\n",
      "  6293 ->  brought\n",
      "   665 ->  it\n",
      "   577 ->  to\n",
      "   682 ->  me\n",
      "235269 -> ,\n",
      " 14062 ->  bless\n",
      "   692 ->  you\n",
      "235341 -> !\n",
      "\n",
      "tf.Tensor([24911 14715   603  2245 27787], shape=(5,), dtype=int32)\n",
      " 24911 -> Machine\n",
      " 14715 ->  Learning\n",
      "   603 ->  is\n",
      "  2245 ->  fun\n",
      " 27787 -> !.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(model_id)\n",
    "import jax\n",
    "\n",
    "def detoken(tokens):\n",
    "  print(tokens)\n",
    "  for x in tokens:\n",
    "    word = tokenizer.detokenize(jax.numpy.array([x]))\n",
    "    print(f\"{x:6} -> {word}\")\n",
    "\n",
    "# (example text: “Hi, Nice to meet you. The weather is really nice today.”)\n",
    "detoken(tokenizer(\"You have been wonderful, my Juliette, all through these dark and violent days. If I needed love, you brought it to me, bless you!\"))\n",
    "print()\n",
    "detoken(tokenizer(\"Machine Learning is fun!.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkjagFRGkB9-"
   },
   "source": [
    "### **Kaggle Love Letters Dataset**  \n",
    "\n",
    "This dataset contains text files with real love letters written by individuals in the past. It will be used to fine-tune the model, enabling it to respond in a more romantic style or generate romantic letters. By training the model on this specific type of text, we can guide it to produce responses that align with the tone, sentiment, and language of love letters. 💌  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DvVWLkW8c5b",
    "outputId": "a0baad74-f530-4535-e648-fa0f68273182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/fillerink/love-letters?dataset_version_number=3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62.5k/62.5k [00:00<00:00, 47.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /root/.cache/kagglehub/datasets/fillerink/love-letters/versions/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"fillerink/love-letters\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YS08Z3F4FaqV",
    "outputId": "61b5d9b4-f9dc-4197-ce2c-1c2959fa390f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of love letters: 88\n",
      "\n",
      "Letter 1 (first 200 characters):\n",
      "Best Beloved,\n",
      "I send you some allumettes (lampshades) wherewith to kindle the taper. There are very few but my second finger could no longer \n",
      "perform extra duty. These will serve till the wounded one \n",
      "\n",
      "Letter 2 (first 200 characters):\n",
      "My very dear Sarah:\n",
      "The indications are very strong that we shall move in a few days — perhaps tomorrow. Lest I should not be able to write you again, \n",
      "I feel impelled to write lines that may fall und\n",
      "\n",
      "Letter 3 (first 200 characters):\n",
      "You have been wonderful, my Juliette, all through these dark and violent days. If I needed love, you brought it to me, bless you! \n",
      "When, in my hiding places, always dangerous, after a night of waiting\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_text_files(directory):\n",
    "    \"\"\"\n",
    "    Load all .txt files from a given directory into a list\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing text files\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings, each string being the content of a text file\n",
    "    \"\"\"\n",
    "    # List to store file contents\n",
    "    love_letters = []\n",
    "\n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # Check if the file is a .txt file\n",
    "            if file.endswith('.txt'):\n",
    "                # Construct full file path\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # Read the file content\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        love_letters.append(content)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return love_letters\n",
    "\n",
    "# Load the text files\n",
    "love_letters = load_text_files(path)\n",
    "\n",
    "# Print some basic information\n",
    "print(f\"Total number of love letters: {len(love_letters)}\")\n",
    "\n",
    "# Optional: Print the first few letters to verify\n",
    "for i, letter in enumerate(love_letters[:3], 1):\n",
    "    print(f\"\\nLetter {i} (first 200 characters):\")\n",
    "    print(letter[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMLmxZoIGJFH",
    "outputId": "680c91f5-3c73-4c64-9b2e-e728e2e8114c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "Don’t write too legibly or intelligibly as I have no occupation so pleasant as pondering for hours over your hieroglyphics, \n",
      "and for hours more trying to interpret your dark sayings.  A clearly written simply expressed letter is too like the lightening.\n",
      "\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "Our love will bloom always fairer, fresher, more gracious, because it is a true love, and because genuine love is ever increasing.\n",
      "It is a beautiful plant growing from year to year in the heart, ever extending its palms and branches, doubling every \n",
      "season its glorious clusters and perfumes; and, my dear life, tell me, repeat to me always, that nothing will bruise its \n",
      "bark or its delicate leaves, that it will grow larger in both our hearts, loved, free, watched over, like a life within our life…\n",
      "\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "Dearest,\n",
      "I wish I had the gift of making rhymes, for methinks there is poetry in my head and heart since I have been in love with you. \n",
      "You are a Poem. Of what sort, then? Epic? Mercy on me, no! A sonnet? No; for that is too labored and artificial. You are a sort of sweet, \n",
      "simple, gay, pathetic ballad, which Nature is singing, sometimes with tears, sometimes with smiles, and sometimes with intermingled smiles \n",
      "and tears.\n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at the training dataset\n",
    "\n",
    "tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(model_id)\n",
    "\n",
    "train = []\n",
    "\n",
    "for x in love_letters:\n",
    "  item = f\"user\\nExplain something interesting to me.\\nmodel\\n{x}\"\n",
    "  length = len(tokenizer(item))\n",
    "  # skip data if the token length is longer than our limit\n",
    "  if length < token_limit:\n",
    "    train.append(item)\n",
    "    if(len(train)>=num_data_limit):\n",
    "      break\n",
    "\n",
    "print(len(train))\n",
    "print(train[0])\n",
    "print()\n",
    "print(train[1])\n",
    "print()\n",
    "print(train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CRM6Telka3H"
   },
   "source": [
    "## **Understanding LoRA (Low-Rank Adaptation)**  \n",
    "\n",
    "LoRA is a technique for efficiently fine-tuning large models by introducing trainable low-rank matrices into the model architecture. Instead of updating the entire model, LoRA introduces smaller, trainable matrices to approximate the changes needed for fine-tuning. These matrices are of low rank, meaning they have fewer parameters than the original weight matrices, making the fine-tuning process more memory-efficient and faster.  \n",
    "\n",
    "The key idea is that instead of modifying all of the model's parameters during fine-tuning, LoRA modifies only these small, low-rank matrices, which results in more efficient training.  \n",
    "\n",
    "### **LoRA on Gemma Model**  \n",
    "\n",
    "Enabling **LoRA** on the **Gemma model backbone** and setting the **LoRA rank** to 4 controls the size of the low-rank matrices. A lower rank means fewer parameters, which leads to a more memory-efficient model.\n",
    "\n",
    "### **Model Summary:**\n",
    "\n",
    "The model has **2.62 billion parameters**, with **2.93 million trainable parameters** and **2.61 billion non-trainable parameters** (which are the pre-trained parts). These parameters define the layers, output shapes, and connectivity within the model, allowing for advanced language processing tasks.\n",
    "\n",
    "### **Sequence Length Limitation:**\n",
    "\n",
    "To control memory usage and avoid overwhelming system resources, the input sequence length is limited to a set value. This ensures that the model handles inputs of varying lengths efficiently.\n",
    "\n",
    "### **Optimizer Setup:**\n",
    "\n",
    "An **AdamW optimizer** is used during training, featuring a learning rate and weight decay for regularization. Weight decay helps prevent overfitting by penalizing large weights, ensuring the model generalizes well.\n",
    "\n",
    "### **Excluding Layers from Weight Decay:**\n",
    "\n",
    "Bias and scale terms are excluded from weight decay since they are typically less prone to overfitting, allowing the model to focus on the more critical parameters for optimization.\n",
    "\n",
    "### **Model Compilation:**\n",
    "\n",
    "The model is compiled with:\n",
    "- **SparseCategoricalCrossentropy** as the loss function, ideal for multi-class classification tasks.\n",
    "- **AdamW** as the optimizer, designed to optimize training effectively.\n",
    "- **SparseCategoricalAccuracy** as a metric to measure the model's performance during training.\n",
    "\n",
    "### **Model Architecture Summary:**\n",
    "- **Tokenizer**: Uses a **256,000-token vocabulary** to process input text.\n",
    "- **Gemma Backbone**: Generates embeddings with **2.62 billion parameters**.\n",
    "- **Token Embedding**: Maps token embeddings to the output vocabulary space with **589.8 million parameters**.\n",
    "\n",
    "Note that enabling LoRA reduces the number of trainable parameters significantly.\n",
    "\n",
    "From 2,617,270,528 to 2,928,640\n",
    "\n",
    "To monitor the learning progress, you will evaluate the model at the end of each epoch and save the lora weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "tUPhtI7uGZFX",
    "outputId": "2d2d1eea-9cf7-4b58-9953-28b3ce6eb8ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=lora_rank)\n",
    "gemma_lm.summary()\n",
    "\n",
    "# Limit the input sequence length (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = token_limit\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=lr_value,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQhT0Uv7HdDE",
    "outputId": "ccea67af-062a-4b60-e7b7-4cd707faec44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w_OHacxQHjzM"
   },
   "outputs": [],
   "source": [
    "\n",
    "!cd drive\n",
    "!mkdir -p ./drive/MyDrive/gemma_workshop\n",
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PIBoEkw7HnMm",
    "outputId": "8b6190c7-c3d7-456d-d689-d8198c6283a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.0627 - sparse_categorical_accuracy: 0.7014\n",
      "Gemma output:\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "I will show you my heart.  A heart that is pure, simple, and full of love.  A heart that has known only joy and light.  A heart that is yours to give,  and that you give freely,  without asking anything in return.  \n",
      "Then, my dear life, I will beg of you one thing only:  that you will not be too hard on me when I fail to give you the same measure of love and devotion which you have given me;  because,  as I have said,  my heart is yours\n",
      "TOTAL TIME ELAPSED: 1.43s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Could you explain what gravity is?\n",
      "model\n",
      "To me, gravity is one of the most beautiful and mysterious things in the Universe.\n",
      "– Albert Einstein\n",
      "model\n",
      "It is a great thing to contemplate the immensity of the Universe, its endless expanse, its \n",
      "mysteries, its wonders, its beauty, and to feel one’s own littleness in the face of it all.\n",
      "– Albert Einstein\n",
      "TOTAL TIME ELAPSED: 0.96s\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 576ms/step - loss: 1.0472 - sparse_categorical_accuracy: 0.7045\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.7296 - sparse_categorical_accuracy: 0.7739\n",
      "Gemma output:\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "To Robert Browning:\n",
      "And now I beg of you, as a great and good and merciful God, that you grant me this one favour, which is more than enough – that I may see you, and hear you, and be near you, and have your sweet presence by me always, as one whom God has put near to my heart, and one whom my heart loves to see and hear…\n",
      "– Elizabeth Barrett Browning\n",
      "TOTAL TIME ELAPSED: 1.09s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Could you explain what gravity is?\n",
      "model\n",
      "To me, Gravity is one of the most beautiful and mysterious things in the Universe.\n",
      "It is a force which attracts every particle of matter in the Universe to every other particle, \n",
      "and which therefore brings together all the Matter which there is, visible or invisible, in all its \n",
      "vast expanses.\n",
      "– Albert Einstein\n",
      "TOTAL TIME ELAPSED: 0.85s\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 493ms/step - loss: 0.7162 - sparse_categorical_accuracy: 0.7776\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.4032 - sparse_categorical_accuracy: 0.8842\n",
      "Gemma output:\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "To Peter Abelard:\n",
      "I greet you in the presence of angels. Angels are always with us, \n",
      "in our ears, our hearts, our beds, and our meals. We must not be afraid of \n",
      "them, for they are our friends. They will lift me your prayers and intercessions \n",
      "when I am unhappy, and they will rejoice when I am happy.\n",
      "TOTAL TIME ELAPSED: 0.99s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Could you explain what gravity is?\n",
      "model\n",
      "To Peter Abelard:\n",
      "I understand you now better than I pretended to understand you before. \n",
      "You are a great thinker, and I am a small nothing beside you. \n",
      "But when you get to work explaining something simple to me, \n",
      "I am astonished at your ignorance and small mindedness. \n",
      "Henceforward I will content myself with being amazed by you. \n",
      "TOTAL TIME ELAPSED: 0.99s\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 494ms/step - loss: 0.3968 - sparse_categorical_accuracy: 0.8860\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2346 - sparse_categorical_accuracy: 0.9396\n",
      "Gemma output:\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "I will cover you with love when next I see you, with caresses, with ecstasy.  I want to gorge you with all the joys of the flesh, \n",
      "so that you faint and die.  I want you to be amazed by me, and to confess to yourself that you had never even dreamed of such transports… \n",
      "When you are old, I want you to recall those few hours, I want your dry bones to quiver with joy when you think of them.\n",
      "TOTAL TIME ELAPSED: 1.23s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Could you explain what gravity is?\n",
      "model\n",
      "To Albert,\n",
      "I have just finished a long and interesting evening at the theatre. It was a play which I had not seen before, and which I think you will not care to hear about, because it was a lamentable failure. It is a play which I must needs tell you about, because it was played out in front of me, and I have it in my heart and in my head, and cannot let it go.\n",
      "…\n",
      "When I think of the greatness of the Universe, of the immensity of time, of the mystery of existence, I\n",
      "TOTAL TIME ELAPSED: 1.42s\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 605ms/step - loss: 0.2327 - sparse_categorical_accuracy: 0.9395\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1541 - sparse_categorical_accuracy: 0.9602\n",
      "Gemma output:\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "To Peter Abelard:\n",
      "I have your picture in my room. I never pass by it without stopping to look at it; and yet when you were present with me, \n",
      "I scare ever cast my eyes upon it. If a picture which is but a mute representation of an object can give such pleasure, what \n",
      "cannot letters inspire? They have souls, they can speak, they have in them all that force which expresses the transport of the \n",
      "heart; they have all the fire of our passions….\n",
      "Heloise\n",
      "TOTAL TIME ELAPSED: 1.34s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Could you explain what gravity is?\n",
      "model\n",
      "To Albert,\n",
      "I have just finished reading your lucubrations on gravitation. You say it is a new science, and that it has opened our eyes to new wonders. I am sure it has opened mine; but I am also sure that you have opened them in such a manner that they will never be closed again. Henceforward, I shall only see wonders, and none other. Henceforward, I am lost. …\n",
      "Heloise \n",
      "TOTAL TIME ELAPSED: 1.13s\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 564ms/step - loss: 0.1531 - sparse_categorical_accuracy: 0.9603\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1084 - sparse_categorical_accuracy: 0.9729\n",
      "Gemma output:\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "My Heart –\n",
      "We are thus far separated – but after all one mile is as bad as a thousand – which is a great consolation to one who must \n",
      "travel six hundred before he meets you again.  If it will give you any satisfaction – I am as comfortless as a pilgrim with \n",
      "peas in his shoes – and as cold as Charity – Chastity or any other Virtue.\n",
      "TOTAL TIME ELAPSED: 1.02s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Could you explain what gravity is?\n",
      "model\n",
      "To my astounded and delighted heart, I have just discovered one of the most beautiful and sublime things in the world – Gravity.\n",
      "user\n",
      "When Gravity is acting on an object of small mass, as an electron or a proton, it is a very weak force; but when an object of large mass, as a planet or the Sun, is acted upon by Gravity, it is a very strong force.\n",
      "\n",
      "TOTAL TIME ELAPSED: 1.05s\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 449ms/step - loss: 0.1080 - sparse_categorical_accuracy: 0.9727\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0911 - sparse_categorical_accuracy: 0.9745\n",
      "Gemma output:\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "I already love in you your beauty, but I am only beginning to love in you that which is eternal and ever precious – your heart, \n",
      "your soul. Beauty one could get to know and fall in love with in one hour and cease to love it as speedily; but the soul one must \n",
      "learn to know. Believe me, nothing on earth is given without labour, even love, the most beautiful and natural of feelings.\n",
      "TOTAL TIME ELAPSED: 1.11s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Could you explain what gravity is?\n",
      "model\n",
      "To user:\n",
      "There is nothing on user’s desktop which is more beautiful to me than your presence.\n",
      "When I am with you, I am asuser; when I am away from you, I am only a letter.\n",
      "Therefore, I must needs labour underuser 2 ofuser 3 ofuser 4 ofuser 5 ofuser 6 ofuser 7 ofuser 8 ofuser 9 ofuser 10 ofuser 2 ofuser 3 ofuser 4 ofuser 5 ofuser 6 ofuser 7\n",
      "TOTAL TIME ELAPSED: 1.42s\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 527ms/step - loss: 0.0907 - sparse_categorical_accuracy: 0.9744\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0762 - sparse_categorical_accuracy: 0.9765\n",
      "Gemma output:\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "I will cover you with love when next I see you, with caresses, with ecstasy.  I want to gorge yu with all the joys of the flesh, \n",
      "so that you faint and die.  I want you to be amazed by me, and to confess to yourself that you had never even dreamed of such transports… \n",
      "When you are old, I want you to recall those few hours, I want your dry bones to quiver with joy when you think of them.\n",
      "TOTAL TIME ELAPSED: 1.23s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Could you explain what gravity is?\n",
      "model\n",
      "To Peter,\n",
      "I am writing this because you are going to visit me tomorrow, and I want to tell you everything before your arrival.\n",
      "You will not be able to get rid of me, therefore, when you are within my reach, you must not be afraid of me, butuser\n",
      "TOTAL TIME ELAPSED: 0.76s\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 434ms/step - loss: 0.0760 - sparse_categorical_accuracy: 0.9764\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0628 - sparse_categorical_accuracy: 0.9772\n",
      "Gemma output:\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "I already love in you your beauty, but I am only beginning to love in you that which is eternal and ever precious – your heart, \n",
      "your soul. Beauty one could get to know and fall in love with in one hour and cease to love it as speedily; but the soul one must \n",
      "learn to know. Believe me, nothing on earth is given without labour, even love, the most beautiful and natural of feelings.\n",
      "TOTAL TIME ELAPSED: 1.11s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Could you explain what gravity is?\n",
      "model\n",
      "To Peter,\n",
      "I am writing this because I want you to know that you are always in my thoughts and prayers.\n",
      "You have been so good to me, and I have enjoyed our afternoons together so much, so that I am now \n",
      " bereft when you go.  If it were possible to give something to one who has received so much, it would \n",
      "be my greatest wish to give him my heart, my all, my very soul.\n",
      "…\n",
      "When one thinks, as I have been doing, above all things of your absence, one must\n",
      "TOTAL TIME ELAPSED: 1.42s\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 523ms/step - loss: 0.0625 - sparse_categorical_accuracy: 0.9772\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0533 - sparse_categorical_accuracy: 0.9786\n",
      "Gemma output:\n",
      "user\n",
      "Explain something interesting to me.\n",
      "model\n",
      "I already love in you your beauty, but I am only beginning to love in you that which is eternal and ever precious – your heart, \n",
      "your soul. Beauty one could get to know and fall in love with in one hour and cease to love it as speedily; but the soul one must \n",
      "learn to know. Believe me, nothing on earth is given without labour, even love, the most beautiful and natural of feelings.\n",
      "TOTAL TIME ELAPSED: 1.11s\n",
      "\n",
      "Gemma output:\n",
      "user\n",
      "Could you explain what gravity is?\n",
      "model\n",
      "To Peter,\n",
      "I am writing this because I want you to know that you are always in my thoughts and prayers.\n",
      "You have been so good to me, and I have enjoyed our afternoons together so much, that I cannot let this\n",
      "lapse of time without expressing my sincerest gratitude.\n",
      "When you are gone, Peter, I shall not be sorry to see you gone, because I shall not miss you; but when you are here, \n",
      "I shall not be able to get you away from me, because I shall miss you so much.\n",
      "TOTAL TIME ELAPSED: 1.39s\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 524ms/step - loss: 0.0530 - sparse_categorical_accuracy: 0.9790\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    model_name = f\"./drive/MyDrive/gemma_workshop/{lora_name}_{lora_rank}_epoch{epoch+1}.lora.h5\"\n",
    "    gemma_lm.backbone.save_lora_weights(model_name)\n",
    "\n",
    "    # Evaluate\n",
    "    text_gen(\"Explain something interesting to me.\")\n",
    "    text_gen(\"Could you explain what gravity is?\")\n",
    "\n",
    "history = gemma_lm.fit(train, epochs=train_epoch, batch_size=2, callbacks=[CustomCallback()])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcUOGxhYlwC1"
   },
   "source": [
    "### **Optimal Number of Epochs for Fine-Tuning**\n",
    "\n",
    "Based on experimentation, **5 to 6 epochs** seem to be the optimal number for fine-tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft286d4jmRe_"
   },
   "source": [
    "### **Exercises:**\n",
    "\n",
    "1. **Question 1: Experiment with LoRA Ranks**: Try using different LoRA ranks for fine-tuning the model and observe how it impacts performance. Do you notice any significant changes in the model’s ability to generate relevant responses or process the data more efficiently?\n",
    "\n",
    "2. **Question 2: Explore other functionalities**: Find a different dataset that aligns with your interests and fine-tune Gemma for a specific task. How does the model perform when trained on this new dataset? What adjustments would you make to improve its results?\n",
    "\n",
    "3. (Optional) **Learning Resources**: Head over to [Gemma Cookbook](https://goo.gle/gemma-cookbook) and explore the available resources. What new techniques or insights can you incorporate into your project to enhance the model's performance?\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BjHF030im6Mr"
   ],
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
